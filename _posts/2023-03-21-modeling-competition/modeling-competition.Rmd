---
title: "Getting 3rd in Modeling Competition"
description: |
  Somehow I got on the podium for a model-building competition in my data science class
author:
  - name: Grant Li
date: 2023-03-21
output:
  distill::distill_article:
    self_contained: false
    toc: true
base_url: https://www.remgrandt.com/posts/2023-03-21-modeling-competition/
---

In the data science class I'm taking, they hosted a model-building competition across all three sections of the class, meaning there were at least 150 students participating, maybe 200. We had to build a model to predict the amount of money a bank would make back on a loan. The goal was to build a linear regression model that would produce the lowest RMSE.

> RMSE is Root Mean Squared Error. Basically you can think of it as a statistical measure to see how accurate your model is. In general you want it smaller.

### On the wrong track

At the start, I did VIF testing on all the continuous variables. The VIF testing showed that none of them were egregiously multicollinear, only 2 or 3 of them were in the 5-10 range, which our professors had said was not good, but can be accepted nevertheless.  

> VIF stands for Variable Inflation Factor, which basically just measures how much multicollinearity there is between your variables. Multicollinearity is where two variables are highly correlated, when really the variables in the model should be independent. VIF testing will give what you can think of as a multicollinearity rating for each variable, and usually having the number be below 5 means that it is good, whereas numbers from 5-10 are suspect, and above 10 means that the variable is highly correlated with another. The lower the number, the better.

Because our variable subsetting algorithms didn't allow for categorical variables, I simply tried making models by running forward stepwise variable subsetting and then K-folds cross validation on the continuous variables and then adding categorical variables into the model after. I tried this for a day or two and it was clear that my RMSE wasn't going to get under the 650 threshold that we had to get under for full credit on this assignment. It was stuck at 800 for a while.

> Without getting too into the nitty-gritty, forward stepwise variable subsetting is a algorithm that allows you to pick the best set of predictors from your variables. K-folds cross validation is a method which helps compare and select models. 

### On the right track

I decided that instead, I would try turn all the categorical variables into dummies, and do it all over again.  

> A dummy variable is essentially turning, for example, a variable that has yes or no responses into two separate columns of yes and no, with 1s and 0s as the response. This way, the data becomes numerical and can be put into functions that otherwise only took continuous/numerical variables. Additionally, it allows the model to choose only the dummy variables that matter, like perhaps only the yes column with 1s and 0s is statistically significant, and the no column with 1s and 0s is not.

```{r, echo = FALSE, fig.align = 'center', out.width = '25%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/vars.png'))
```

I technically should have done exploratory data analysis (EDA) and picked a few variables that would have worked best. But since my computer could handle it, I turned every single categorical variable into dummy variables, except emp_title and earliest_cr_line, since those had just way too many categories that it honestly wasn't worth it. Even then, the forward stepwise variable subsetting took 20 minutes each time. While I ended up pursuing the brute-force method, I did end up doing EDA at the end which basically corroborated everything that was produced below.

Since our k-folds cross validation function only takes 4 variables, I used the model with the 4 best predictors to find what I should put into the k-folds cross validation function. The best 4 variables were out_prncp_inv, loan_amnt, int_rate, and term_36_months, which is a dummy variable created from the term variable. 

```{r, echo = FALSE, fig.align = 'center', out.width = '60%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/best4.png'))
```

Then I put this through k-folds cross-validation, which added selected interactions. The k-folds cross validation was pretty impressive, because after working on this for days, it kind of stunned me to just watch the RMSE keep going down like that all the way to the 400s.

```{r, echo = FALSE, fig.align = 'center', out.width = '70%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/kfolds.png'))
```

These were the interactions that the k-folds cross validation found.

```{r, echo = FALSE, fig.align = 'center', out.width = '100%', preview = TRUE}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/selected.png'))
```

I then used the BIC curve to find the optimal number of predictors from the most recent forward stepwise variable subsetting. 

> BIC stands for Bayesian information criterion. The BIC curve in this case stands for a curve that after variable subsetting, shows you what it believes to be the most ideal number of predictors is. 

```{r, echo = FALSE, fig.align = 'center', out.width = '50%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/bic.png'))
```

The curve says 33, but I found the 32 best predictors, not that the difference really matters that much. Here are some of them, since I literally can't fit them in my screenshot.

```{r, echo = FALSE, fig.align = 'center', out.width = '50%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/preds32.png'))
```

I tested each one of them by adding them into the model formula that we got from the k-folds cross validation, and saw if it decreased RMSE. After doing so, the ones that decreased the RMSE by any appreciable amount were grade_a and total_rec_late_fee. The final model formula looked like this, including the selected interactions earlier:

```{r, eval = FALSE}
# K-folds model with new predictors after testing predictors that appear in the 
# 32 predictor model

model = sm.ols('money_made_inv ~ 
               out_prncp_inv + 
               loan_amnt + 
               int_rate + 
               term_36_months + 
               grade_a + 
               total_rec_late_fee' + 
                 selected_interactions, data = trainCopy).fit()
```


After submitting this, it turns out that I was in the top 3 student submissions!

```{r, echo = FALSE, fig.align = 'center', out.width = '100%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/pubRank.png'))
```
Number 1 is a professor, Arvind Krishna, so he doesn't count. This was the public submission leaderboard, which only calculated the RMSE on 30% of the test data. I had no clue until the competition ended that there was also a private submission leaderboard that calculated the RMSE on 70% of the test data. That was the actual "final" ranking, where I was 9th, which still really good. 

```{r, echo = FALSE, fig.align = 'center', out.width = '100%'}
knitr::include_graphics(here::here('_posts/2023-03-21-modeling-competition/privRank.png'))
```

Nevertheless, I kind of like my 3rd place finish on the public leaderboard more, not because I was 3rd as opposed to 9th, but because there were things like outlier removal, which did not lower the RMSE on the public submission board so I didn't include it in my final submission, but could have on the private one if I could only have just seen the RMSE.

### What more could I have done?

I could have also done some regularization with lasso or ridge.

> Lasso and ridge help balance out the coefficients, and prevent the model from overfitting.